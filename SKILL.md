---
name: fhir-connectathon-notes
description: Testing toolkit for the FHIR Writing Clinical Notes specification at connectathons. Use when the user needs to test FHIR DocumentReference write operations, validate conformance with the Writing Clinical Notes spec, or participate in a FHIR connectathon for clinical notes. Includes templates, OAuth helpers, and automated test scenarios for both provider-authored and patient-asserted notes.
---

# FHIR Connectathon Notes Testing Skill

Comprehensive toolkit for testing the FHIR Writing Clinical Notes specification at connectathons and in development environments.

## Quick Start

1. **Set up project structure** - Copy template files to project root
2. **Run the bun setup script** - Launch the interactive setup wizard
3. Localize note templates to target server's patient/encounter IDs
4. Execute test scenarios in sequence
5. Validate responses against spec requirements

## IMPORTANT: First Steps for the Agent

Before proceeding with any testing tasks, the agent MUST:

### 1. Set Up Project Structure

**Check if project structure already exists:**

```bash
# Check for existing setup
if [ -f ".gitignore" ] && [ -d ".fhir-configs" ]; then
  echo "âœ“ Project structure already exists"
else
  echo "Setting up project structure..."

  # Copy .gitignore to exclude sensitive configs and debug logs
  cp .claude/skills/fhir-connectathon-notes/assets/project/.gitignore .

  # Create directory structure
  mkdir -p .fhir-configs debug localized

  # Copy fhir-request library to debug/
  cp .claude/skills/fhir-connectathon-notes/assets/lib/fhir-request.ts debug/

  echo "âœ“ Project structure created"
fi
```

This creates:
- `.gitignore` - Excludes `.fhir-configs/`, `debug/`, and `localized/`
- `.fhir-configs/` - Will store server connection configs
- `debug/` - Will store request/response logs organized by server and timestamp
- `localized/` - Will store template-generated resources organized by server
- `FHIR-TESTING.md` - Documentation about the testing structure

**Note:** If files already exist (from previous testing), they will NOT be overwritten.

### 2. Understanding the Specification and Scenarios

The agent has access to the complete specification and test scenarios embedded below.

#### Writing Clinical Notes Specification

@references/spec.md

#### Connectathon Test Scenarios

@references/scenarios.md

### 3. Debug Logging with request.ts

**All FHIR API interactions MUST be logged using the request.ts pattern:**

**Setup (one time per server):**

Each server gets its own directory under `debug/` with a shared `fhir-request.ts` library:

```bash
# Example: Setting up for "smart-demo-open" server
mkdir -p debug/smart-demo-open
cp .claude/skills/fhir-connectathon-notes/assets/lib/fhir-request.ts debug/smart-demo-open/
```

**Directory structure:**
```
debug/
â””â”€â”€ smart-demo-open/                    # Server name (lowercase, hyphenated)
    â”œâ”€â”€ fhir-request.ts                 # Shared library for this server
    â”œâ”€â”€ 2025-11-03T18-17-25-scenario1/  # Timestamped test runs
    â”‚   â”œâ”€â”€ request.ts                  # Test request
    â”‚   â”œâ”€â”€ response-metadata.json      # Generated by library
    â”‚   â””â”€â”€ response-body.json          # Generated by library
    â””â”€â”€ 2025-11-03T18-22-43-scenario2/
        â”œâ”€â”€ request.ts
        â”œâ”€â”€ response-metadata.json
        â””â”€â”€ response-body.json
```

**For each FHIR request:**

1. **Create debug directory** with timestamp and purpose:
   ```bash
   TIMESTAMP=$(date -u +"%Y-%m-%dT%H-%M-%S")
   DEBUG_DIR="debug/smart-demo-open/${TIMESTAMP}-scenario1-create"
   mkdir -p "$DEBUG_DIR"
   ```

2. **Write `request.ts`** in that directory:
   ```typescript
   import { execute } from '../fhir-request.ts';

   await execute({
     method: "POST",
     path: "/DocumentReference",
     bodyFile: "localized/smart-demo-open/consultation-note.json",
     headers: {
       // Optional: additional headers like "If-None-Exist": "identifier=system|value"
     },
     purpose: "Scenario 1: Basic document creation",
     callerDir: import.meta.dir  // CRITICAL: Tells library where to write output files
   });
   ```

3. **Run from PROJECT ROOT** (not from the debug directory):
   ```bash
   bun run debug/smart-demo-open/2025-11-03T18-17-25-scenario1-create/request.ts
   ```

   **Why from project root?**
   - The library needs to find `.fhir-configs/` (at project root)
   - The `bodyFile` path is relative to project root
   - The `import.meta.dir` in request.ts gives the library the debug directory to write to

4. **Output files are automatically created as siblings to request.ts:**
   - `response-metadata.json` - HTTP status, headers, timing
   - `response-body.json` (or `.txt`) - Response body

**CRITICAL PATH RULES:**

âœ… **DO THIS:**
```bash
# Always run from project root
cd /path/to/project
bun run debug/server-name/timestamp-purpose/request.ts
```

âŒ **DON'T DO THIS:**
```bash
# Don't cd into debug directory
cd debug/server-name/timestamp-purpose
bun request.ts  # Will fail to find .fhir-configs
```

**Key Points:**
- `fhir-request.ts` lives at `debug/{server-name}/fhir-request.ts` (one per server)
- `request.ts` files live at `debug/{server-name}/{timestamp}-{purpose}/request.ts`
- Always use `callerDir: import.meta.dir` in your execute() call
- Always run from project root: `bun run debug/path/to/request.ts`
- The library uses `callerDir` to write response files as siblings to `request.ts`

**Benefits:**
- Clean, readable request specification
- No path confusion - always run from project root
- Automatic config loading and auth handling
- Consistent logging every time
- Easy to replay/modify (just edit and re-run)
- Request is self-documenting

### 4. Note Configuration Storage Location

Server configurations are stored in `.fhir-configs/` in the project root (NOT in the skill directory). This makes them accessible across different skill invocations and preserves them between sessions.

---

## Setup: Authentication & Configuration

### Step 1: Launch the Interactive Setup (Bun Script)

The easiest way to get started is to use the bun setup script:

1. **IMPORTANT**: Run setup in the **foreground** (NOT as a background process) so you can immediately see when it completes:
   ```bash
   bun .claude/skills/fhir-connectathon-notes/assets/config/setup.ts
   ```
2. The script will print a clickable link to http://localhost:3456
3. Open the link in your browser
4. Choose to create a new configuration or select an existing one
5. Follow the interactive wizard to configure your FHIR server connection
6. Watch for the output line: `ðŸ“‹ SELECTED_CONFIG: {name}`
7. Configuration will be automatically saved and the server will shut down

The setup script will:
- Start a local web server (prints link to CLI)
- Display existing configurations you can switch between
- Guide you through creating new FHIR server configurations
- Handle OAuth flows or manual token entry
- Save named configurations to `.fhir-configs/` directory in the project root
- Output the selected config name to stdout with `ðŸ“‹ SELECTED_CONFIG: {name}`
- Automatically exit when setup is complete

### Step 2: Configure Server Connection

The setup wizard supports four authentication modes:

**Mode 1: Manual Token** (fastest for testing)
- Paste an existing access token
- Enter FHIR base URL
- Optionally provide patient ID and encounter reference
- Click "Use This Configuration"

**Mode 2: Open Server** (no authentication)
- Enter FHIR base URL
- Optionally provide patient ID and encounter reference
- Click "Use Open Server"
- Useful for public test servers

**Mode 3: OAuth Public Client**
- Enter FHIR base URL
- Click "Auto-Discover Endpoints" (or manually enter auth/token endpoints)
- Enter client ID
- Click "Start OAuth Flow"
- Complete authentication in browser
- Authorization code is automatically captured and exchanged for token

**Mode 4: OAuth Confidential Client**
- Same as public client, but also requires client secret
- Token exchange happens server-side with credentials

### Step 3: Reading the Selected Configuration

After successful setup, the script outputs:
```
ðŸ“‹ SELECTED_CONFIG: {config-name}
```

The agent should:
1. Capture this config name from the setup script output
2. Read the configuration from `.fhir-configs/{config-name}.json`

Example configuration file structure:
```json
{
  "name": "SMART Open",
  "fhirBaseUrl": "https://fhir.example.org/r4",
  "accessToken": "eyJ...",
  "patientId": "patient-123",
  "encounterRef": "Encounter/enc-456",
  "mode": "public",
  "savedAt": "2025-11-03T10:00:00.000Z"
}
```

### Alternative: Manual Setup (Advanced)

If you prefer not to use the bun script, you can manually create a file in `.fhir-configs/` with the structure above.

## Using Saved Configuration

Once setup is complete, the agent should:

1. **Extract the config name from setup output and read the configuration file:**
   ```bash
   # Extract config name from setup script output
   CONFIG_NAME=$(grep "ðŸ“‹ SELECTED_CONFIG:" setup_output.txt | cut -d: -f2 | tr -d ' ')

   # Read the selected configuration
   cat ".fhir-configs/${CONFIG_NAME}.json"
   ```

2. **Make FHIR requests using the request.ts pattern:**

   For every FHIR API call, use the request.ts template approach (see Debug Logging section above).

   **Example workflow:**

   ```bash
   # 1. Determine server name from config
   SERVER_NAME=$(jq -r '.name' ".fhir-configs/SMART Demo Open.json" | tr '[:upper:] ' '[:lower:]-')
   # Result: "smart-demo-open"

   # 2. Create debug directory with timestamp
   TIMESTAMP=$(date -u +"%Y-%m-%dT%H-%M-%S")
   DEBUG_DIR="debug/${SERVER_NAME}/${TIMESTAMP}-scenario1-create"
   mkdir -p "$DEBUG_DIR"

   # 3. Write request.ts using the fhir-request library
   cat > "$DEBUG_DIR/request.ts" <<'EOF'
import { execute } from '../fhir-request.ts';

await execute({
  method: "POST",
  path: "/DocumentReference",
  bodyFile: "localized/smart-demo-open/consultation-note.json",
  purpose: "Scenario 1: Basic document creation",
  callerDir: import.meta.dir  // CRITICAL: pass the directory to write responses
});
EOF

   # 4. Run the request FROM PROJECT ROOT
   bun run "$DEBUG_DIR/request.ts"
   ```

   The script automatically:
   - Loads your FHIR config from `.fhir-configs/`
   - Makes the fetch request with proper auth
   - Writes `response-metadata.json` and `response-body.json` as siblings to `request.ts`
   - Prints summary with status, resource ID, location header

## Template Localization

Templates are located in `assets/templates/` with placeholder variables:

### Available Templates

**Text/Plain (required by spec):**
1. **consultation-note.json** - Provider consultation with resolvable encounter (text/plain; charset=utf-8)
2. **progress-note.json** - Provider progress note with contained encounter (text/plain; charset=utf-8)
3. **discharge-summary.json** - Hospital discharge summary (text/plain; charset=utf-8)

**PDF (required by spec):**
4. **consultation-note-pdf.json** - Consultation note in PDF format (application/pdf)

**CDA (C-CDA proper format):**
5. **discharge-summary-cda-xml.json** - C-CDA structured discharge summary (application/cda+xml)

**XHTML Rich Text (should support per spec):**
6. **discharge-summary-xhtml.json** - Discharge summary in XHTML format (application/xhtml+xml)

**HTML Rich Text:**
7. **progress-note-html.json** - Progress note with HTML formatting (text/html; charset=utf-8)

**Patient-Asserted:**
8. **patient-asserted-note.json** - Patient-authored health log with PATAST security tag

**Size Limit Testing:**
9. **large-note.json** - Tests 5 MiB size requirement (generated programmatically)

### Placeholder Variables

Each template contains placeholders that must be replaced:

**Patient/Subject:**
- `{{PATIENT_ID}}` - Patient resource ID (e.g., "patient-123")
- `{{PATIENT_NAME}}` - Patient display name (e.g., "John Doe")

**Author:**
- `{{AUTHOR_REFERENCE}}` - Practitioner/PractitionerRole reference (e.g., "Practitioner/dr-smith")
- `{{AUTHOR_DISPLAY}}` - Author display name (e.g., "Dr. Jane Smith")
- `{{APP_NAME}}` - App name for patient-asserted notes

**Encounter:**
- `{{ENCOUNTER_REFERENCE}}` - Encounter resource reference (e.g., "Encounter/visit-789")
- `{{ENCOUNTER_DISPLAY}}` - Encounter display text
- `{{ENCOUNTER_IDENTIFIER_SYSTEM}}` - System for encounter identifier
- `{{ENCOUNTER_IDENTIFIER_VALUE}}` - Encounter identifier value
- `{{ENCOUNTER_START}}` - Encounter start datetime
- `{{ENCOUNTER_END}}` - Encounter end datetime

**Identifier:**
- `{{IDENTIFIER_SYSTEM}}` - System for note identifier (use your app's URL)
- `{{IDENTIFIER_VALUE}}` - Unique identifier value

**Content:**
- `{{BASE64_CONTENT}}` - Base64-encoded note content
- `{{CONTENT_SIZE}}` - Size in bytes of decoded content
- `{{CONTENT_TYPE}}` - MIME type (text/plain; charset=utf-8, application/pdf)

**Timestamps:**
- `{{CURRENT_TIMESTAMP}}` - Current ISO 8601 timestamp
- `{{PERIOD_START}}` - Clinical period start date
- `{{PERIOD_END}}` - Clinical period end date

### Localization Process

When localizing templates for a specific server:

1. **Determine server directory name** from config (e.g., "smart-launcher", "epic-sandbox")
2. **Create server-specific directory:**
   ```bash
   mkdir -p "localized/${SERVER_NAME}"
   ```
3. **Read template file** from `assets/templates/`
4. **Read sample content** from `assets/sample-content/` (if using text)
5. **Base64 encode content:**
   ```bash
   cat assets/sample-content/consultation-note.txt | base64 -w 0
   ```
6. **Check for missing context and ask user for guidance when needed:**

   **IMPORTANT:** If required context is missing or ambiguous, ASK THE USER how to proceed:

   - **No encounter ID in config?** Ask: "I don't see an encounter ID. Should I:
     - Omit encounter references entirely?
     - Create a contained encounter resource?
     - Use a placeholder encounter ID?"

   - **No author/practitioner reference?** Ask: "What should I use for the author reference?
     - A test practitioner ID you provide?
     - A placeholder reference?
     - Skip the author field (if testing allows)?"

   - **Patient name not specified?** Ask: "Should I fetch the patient name from the server, use a placeholder, or omit it?"

   - **Missing identifier system?** Ask: "What identifier system should I use for note identifiers? (e.g., your app's URL)"

   **Don't guess or make assumptions about clinical data.** User input ensures tests are meaningful and aligned with their testing goals.

7. **Replace all placeholders** with values from the configuration and user-provided context
8. **Save to server-specific directory:**
   ```bash
   # Save localized file
   echo "$localized_json" > "localized/${SERVER_NAME}/consultation-note.json"
   ```
9. **Validate JSON** before submission

This keeps each server's localized resources separate and organized.

### Sample Content Files

The skill includes generator scripts for different content types. Run these from the skill's `assets/sample-content/` directory:

**Text Files (pre-created):**
- `consultation-note.txt` - Plain text consultation note
- `progress-note.txt` - Plain text progress note
- `patient-log.txt` - Patient-authored health log

**Generated Content (run these scripts as needed):**
- `bun generate-pdf.ts` - Creates `consultation-note.pdf` (minimal valid PDF)
- `bun generate-cda-xml.ts` - Creates `discharge-summary.cda.xml` (proper HL7 C-CDA with structured body)
- `bun generate-xhtml.ts` - Creates `discharge-summary.xhtml` (XHTML rich text, NOT CDA)
- `bun generate-html.ts` - Creates `progress-note.html` (HTML with rich formatting)
- `bun generate-large-file.ts` - Creates `large-note.txt` (~5 MiB text file)

**Why generate programmatically?**
- Keeps repository size small
- Ensures fresh, customizable test content
- Easy to modify for specific testing needs
- Avoids committing large binary files

**Example usage:**
```bash
cd .claude/skills/fhir-connectathon-notes/assets/sample-content
bun generate-pdf.ts
# Creates consultation-note.pdf in current directory
```

## Test Scenario Execution

Execute scenarios in order. Each builds on previous validations. The complete scenario details are embedded above in the "Connectathon Test Scenarios" section.

### Scenario 1: Basic Document Creation

**Objective:** Verify basic create and retrieval

```bash
# 1. Localize template for this server
SERVER_NAME="smart-launcher"  # from config
template = read(consultation-note.json)
localized = replace_placeholders(template, config)
mkdir -p "localized/${SERVER_NAME}"
echo "$localized" > "localized/${SERVER_NAME}/consultation-note.json"

# 2. POST to server using request.ts pattern
TIMESTAMP=$(date -u +"%Y-%m-%dT%H-%M-%S")
DEBUG_DIR="debug/${SERVER_NAME}/${TIMESTAMP}-scenario1-create"
mkdir -p "$DEBUG_DIR"
cp "debug/${SERVER_NAME}/fhir-request.ts" "$DEBUG_DIR/../" 2>/dev/null || cp .claude/skills/fhir-connectathon-notes/assets/lib/fhir-request.ts "debug/${SERVER_NAME}/"

# Create request.ts for POST
cat > "$DEBUG_DIR/request.ts" << 'EOF'
import { execute } from '../fhir-request.ts';

await execute({
  method: "POST",
  path: "/DocumentReference",
  bodyFile: "localized/smart-launcher/consultation-note.json",
  purpose: "Scenario 1: Basic document creation",
  callerDir: import.meta.dir
});
EOF

# Run the request (from project root)
bun run "$DEBUG_DIR/request.ts"

# 3. Response is automatically captured in:
# - response-metadata.json (status, headers, timing)
# - response-body.json (returned DocumentReference)
# Expect 201 Created or 202 Accepted

# 4. GET to verify (extract ID from response-body.json first)
DOC_ID=$(jq -r '.id' "$DEBUG_DIR/response-body.json")
GET_DIR="debug/${SERVER_NAME}/${TIMESTAMP}-scenario1-read"
mkdir -p "$GET_DIR"

cat > "$GET_DIR/request.ts" << EOF
import { execute } from '../fhir-request.ts';

await execute({
  method: "GET",
  path: "/DocumentReference/${DOC_ID}",
  purpose: "Scenario 1: Verify created document",
  callerDir: import.meta.dir
});
EOF

bun run "$GET_DIR/request.ts"

# 5. Validate response (check response-body.json)
# - All Must Support elements present
# - Content matches original
# - Base64 decodes correctly
```

**Success criteria:**
- âœ“ Server accepts note (201/202)
- âœ“ All elements round-trip
- âœ“ Content retrievable and accurate

### Scenario 2: Conditional Create (Idempotency)

**Objective:** Test duplicate prevention

```bash
# 1. Use same localized note from Scenario 1
# 2. POST with If-None-Exist header using request.ts
TIMESTAMP=$(date -u +"%Y-%m-%dT%H-%M-%S")
DEBUG_DIR="debug/${SERVER_NAME}/${TIMESTAMP}-scenario2-conditional"
mkdir -p "$DEBUG_DIR"

# Extract identifier from the localized note
IDENTIFIER_SYSTEM=$(jq -r '.identifier[0].system' "localized/${SERVER_NAME}/consultation-note.json")
IDENTIFIER_VALUE=$(jq -r '.identifier[0].value' "localized/${SERVER_NAME}/consultation-note.json")

cat > "$DEBUG_DIR/request.ts" << EOF
import { execute } from '../fhir-request.ts';

await execute({
  method: "POST",
  path: "/DocumentReference",
  bodyFile: "localized/${SERVER_NAME}/consultation-note.json",
  headers: {
    "If-None-Exist": "identifier=${IDENTIFIER_SYSTEM}|${IDENTIFIER_VALUE}"
  },
  purpose: "Scenario 2: Conditional create (idempotency test)",
  callerDir: import.meta.dir
});
EOF

bun run "$DEBUG_DIR/request.ts"

# 3. Check response-metadata.json - Expect 200 OK or 304 Not Modified (not 201)

# 4. Search to confirm only one resource exists
SEARCH_DIR="debug/${SERVER_NAME}/${TIMESTAMP}-scenario2-search"
mkdir -p "$SEARCH_DIR"

cat > "$SEARCH_DIR/request.ts" << EOF
import { execute } from '../fhir-request.ts';

await execute({
  method: "GET",
  path: "/DocumentReference?identifier=${IDENTIFIER_SYSTEM}|${IDENTIFIER_VALUE}",
  purpose: "Scenario 2: Verify no duplicate created",
  callerDir: import.meta.dir
});
EOF

bun run "$SEARCH_DIR/request.ts"
# Check response-body.json - should show Bundle with single entry
```

**Success criteria:**
- âœ“ No duplicate created
- âœ“ Existing resource returned
- âœ“ Search shows single resource

### Scenario 3: Status Correction

**Objective:** Test entered-in-error workflow

```bash
# 1. Create a note (use Scenario 1)
# Assume DOC_ID is from Scenario 1's response

# 2. Submit partial update using request.ts
TIMESTAMP=$(date -u +"%Y-%m-%dT%H-%M-%S")
DEBUG_DIR="debug/${SERVER_NAME}/${TIMESTAMP}-scenario3-correction"
mkdir -p "$DEBUG_DIR"

# Create minimal update payload
PATIENT_ID=$(jq -r '.patientId' .fhir-configs/${SERVER_NAME}.json)

cat > "$DEBUG_DIR/update-payload.json" << EOF
{
  "resourceType": "DocumentReference",
  "id": "${DOC_ID}",
  "status": "entered-in-error",
  "subject": {"reference": "Patient/${PATIENT_ID}"}
}
EOF

cat > "$DEBUG_DIR/request.ts" << EOF
import { execute } from '../fhir-request.ts';

await execute({
  method: "PUT",
  path: "/DocumentReference/${DOC_ID}",
  bodyFile: "debug/${SERVER_NAME}/${TIMESTAMP}-scenario3-correction/update-payload.json",
  purpose: "Scenario 3: Mark document as entered-in-error",
  callerDir: import.meta.dir
});
EOF

bun run "$DEBUG_DIR/request.ts"

# 3. GET to verify status change
GET_DIR="debug/${SERVER_NAME}/${TIMESTAMP}-scenario3-verify"
mkdir -p "$GET_DIR"

cat > "$GET_DIR/request.ts" << EOF
import { execute } from '../fhir-request.ts';

await execute({
  method: "GET",
  path: "/DocumentReference/${DOC_ID}",
  purpose: "Scenario 3: Verify status correction",
  callerDir: import.meta.dir
});
EOF

bun run "$GET_DIR/request.ts"
# Check response-body.json - status should be "entered-in-error"

# 4. Test search behavior (may be excluded from results)
```

**Success criteria:**
- âœ“ Partial update accepted
- âœ“ Status changed to entered-in-error
- âœ“ Still readable via direct ID

### Scenario 4: Document Replacement

**Objective:** Test supersession with relatesTo

```bash
# 1. Create initial DocumentReference
original_id = POST /DocumentReference -> capture ID

# 2. Create replacement with relatesTo
replacement = {
  ...original_template,
  "relatesTo": [{
    "code": "replaces",
    "target": {
      "reference": "DocumentReference/{{original_id}}"
    }
  }],
  "content": [{"attachment": {"data": "{{updated_content}}"}}]
}

# 3. POST replacement
POST /DocumentReference with replacement

# 4. Verify both exist
GET /DocumentReference/{{original_id}}
GET /DocumentReference/{{replacement_id}}

# 5. Confirm relatesTo preserved
```

**Success criteria:**
- âœ“ Both documents exist
- âœ“ relatesTo linkage preserved
- âœ“ Both accessible via ID

## Common Issues & Solutions

### Issue: "Authorization failed"
- **Check:** Token is valid and not expired
- **Check:** Scopes include `DocumentReference.c` or `.u`
- **Solution:** Re-run auth helper to get fresh token

### Issue: "Patient not found"
- **Check:** Patient ID matches server's test data
- **Solution:** Use server's documented test patient IDs

### Issue: "Invalid reference"
- **For contained resources:** Use `#id` format
- **For resolvable references:** Ensure resource exists on server
- **Remember:** Servers SHALL NOT reject solely due to unresolvable references

### Issue: "Content too large"
- **Check:** Servers must support at least 5 MiB
- **Solution:** Reduce content size or check server's documented limits

### Issue: "Conditional create not working"
- **Check:** Using `If-None-Exist` header correctly
- **Check:** Identifier system and value match exactly
- **Solution:** Verify header format and identifier uniqueness

## Validation Checklist

Use this checklist to verify server conformance:

**Must Support Elements:**
- [ ] status (current)
- [ ] type (LOINC code)
- [ ] category (clinical-note)
- [ ] subject (Patient reference)
- [ ] content.attachment.contentType
- [ ] content.attachment.data or .url

**Additional Elements:**
- [ ] identifier (for idempotency)
- [ ] author (various reference types)
- [ ] context.encounter (resolvable or contained)
- [ ] context.period
- [ ] date (instant)
- [ ] content.format
- [ ] meta.security (PATAST for patient-asserted)

**Server Capabilities:**
- [ ] Accepts text/plain; charset=utf-8
- [ ] Accepts application/pdf
- [ ] Supports conditional create (If-None-Exist)
- [ ] Accepts â‰¥5 MiB inline content
- [ ] Returns OperationOutcome on errors
- [ ] Allows partial PUT for status correction
- [ ] Accepts and round-trips relatesTo

## References

The complete specification and test scenarios are embedded in this skill document:

**Writing Clinical Notes Specification:**
@references/spec.md

**Connectathon Test Scenarios:**
@references/scenarios.md

These provide all the details about Must Support elements, conformance requirements, and step-by-step testing instructions.

## Tips for Effective Testing

1. **Start simple:** Begin with Scenario 1 using text/plain content
2. **Test variations:** Try different note types (consultation, progress, discharge)
3. **Test edge cases:** Large files, special characters in content, missing optional fields
4. **Document findings:** Note any server-specific behaviors or limitations
5. **Iterate:** Use server feedback to refine your approach
6. **Coordinate:** Share results with other participants via Zulip/Confluence
